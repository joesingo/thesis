\chapter{Introduction}

% \begin{notes}
%     \begin{itemize}
%         \item Overall theme: how should we deal with unreliable information?
%         \item We want to:
%         \begin{itemize}
%             \item Aggregate conflicting reports (crowdsourcing, news)
%             \item Assess the trustworthiness of information sources
%             \item Understand what reliability, trustworthiness and expertise
%                   \emph{mean}
%             \item Find the truth with imperfect information
%         \end{itemize}
%         \item This thesis offers two main perspectives on these general themes
%         \begin{itemize}
%             \item \textbf{Social choice theory.}
%             \begin{itemize}
%                 \item By posing the aggregation problem as one of social
%                       choice, we can apply the axiomatic method to investigate
%                       desirable properties of aggregation methods. We can then
%                       analyse and evaluate such methods in a formal and
%                       principled way.
%                 \item Related ranking problems can be addressed through the
%                       lens of social choice.
%             \end{itemize}
%             \item \textbf{Logic and knowledge representation.}
%             \begin{itemize}
%                 \item We develop a logical system to formalise notions of
%                       expertise, and explore connections with knowledge and
%                       information.
%                 \item We use these formal notions to express the aggregation
%                       problem in logical terms, taking an alternative look at
%                       the problems of the first part of the thesis. We use what
%                       is essentially still an axiomatic approach, but now in
%                       the tradition of knowledge representation and rational
%                       belief change.
%                 \item This logical model is well-suited to investigate
%                       \emph{truth-tracking}: the question of when we can find
%                       the truth given that not all sources are experts.
%             \end{itemize}
%         \end{itemize}
%         \item Note that while there are many links between the two major parts,
%               they are not tightly connected and may be read independently.
%     \end{itemize}
% \end{notes}

The overall theme of this thesis is \emph{unreliable information}. How should
unreliable information be aggregated? Who should be trusted when conflicts
arise between unreliable sources? And what, if anything, can be \emph{learned}
from non-expert information? These are the central issues the thesis aims to
address.

Indeed, methods for understanding and reasoning with unreliable information are
becoming ever more relevant in today's world, as the volume of data produced
and consumed grows year-on-year \todo{c}. With the growth of user-generated
content on the internet, most prominently on social media platforms, false
information can spread rapidly -- sometimes with dramatic
consequences \todo{c}. As such, much research effort has gone into identifying
false information, estimating source reliability, and understanding the nature
of how people may come to believe and share false information \todo{cs}.

The gap this thesis aims to fill concerns \emph{formal models} of problems
surrounding unreliable information. We take a mathematical approach, putting
forward formal frameworks in which the relevant problems can be formulated
precisely. In doing so we obtain conceptual results which aim to shine light on
the core, abstract features of such problems. It is hoped that the thesis
complements the various empirical, practical and philosophical approaches
\todo{c} to our topic, and contributes to the broader understanding of
trustworthiness, expertise and unreliable information from a mathematical point
of view.

The thesis is split into two parts along methodological lines. In
\cref{chapter_td,chapter_bipartite_tournaments} we use the tools and ideas of
\emph{social choice theory}~\cite{zwicker2016voting} to explore the problems of
aggregating unreliable information and ranking sources by trustworthiness.
%
% We employ the \emph{axiomatic method}, in which intuitively desirable
% properties called \emph{axioms} are introduced and studied. Aggregation and
% ranking methods can then be evaluated and compared with respect to these
% axioms, often leading to fresh insights on both individual methods and the
% nature of the problem itself.
%
\cref{chapter_expertise,chapter_belief_change,chapter_truthtracking} take a
\emph{logic-based} approach. We develop a modal logic framework to give precise
semantics for expertise, and explore the connection between expertise,
knowledge, and the truthfulness of information. This framework is used as the
foundation for a belief change problem in the tradition of knowledge
representation and rational belief change \todo{c}. Finally, we combine ideas
from \emph{formal learning theory} \todo{c} (and in particular, the
intersection of formal learning theory and belief revision \todo{c}) to
investigate the extent to which one can \emph{learn} from unreliable
information.
%
In the remainder of this chapter we briefly survey the literature for both
halves, and outline the main contributions of the thesis.

\section{Social Choice Perspectives}
\label{intro_sec_social_choice_perspectives}

Broadly speaking, social choice theory is the study of \emph{aggregating
preferences}. The prototypical example is \emph{voting}. In an election, each
member of the electorate submits a vote in the form of their preferences over
the candidates. A voting rule then aggregates these preferences into a
\emph{collective decision} by declaring the winning candidate, the runner-up,
and so on. There are, of course, many different voting methods which can be
used to aggregate votes, and several distinct methods are in use in different
contexts across the world \todo{c}.

In analysing and comparing such methods, the \emph{axiomatic approach} has been
a crucial methodological tool since the seminal work of \textcite{arrow1952},
who initiated the age of so-called \emph{classical social choice theory}. In
taking this approach, one formalises intuitively desirable properties called
\emph{axioms}, which are expected to hold for ``reasonable'' aggregation
methods. The benefits of this approach were already shown by Arrow, who proved
it is mathematically impossible for any voting method to simultaneously satisfy
a short list of seemingly desirable axioms. This type of result -- known as an
\emph{impossibility theorem} -- highlights a fundamental and inescapable
property of voting.\footnotemark{} This has practical consequences: if one
needs to actually choose a rule for use in a vote, which axiom will be
sacrificed? The axioms also provide a normative basis on which to judge and
compare different voting rules.

The axiomatic approach has since been adapted to various domains besides
voting, including tournaments~\cite{brandt2016a}, judgement
aggregation~\cite{endriss2016ja}, the ranking of web
pages~\cite{altman2005ranking}, reputation systems~\cite{tennenholtz2004} and
collective annotation~\cite{kruger2014}.

\footnotetext{
    At least, a property of voting in the sense of Arrow's formal framework.
}

In the last 20 years, \emph{computational social choice}~\cite{moulin_2016} has
combined social choice theory with ideas from theoretical computer science. For
example, complexity theory has been used to show certain voting rules are
resistant to strategic manipulation, approximation algorithms have been
developed for computationally difficult rules, and SAT solvers have been used
to automatically discover new impossibility theorems \todo{cs}.

In the spirit of combining the axiomatic approach with computer science, we
proceed in \cref{chapter_td,chapter_bipartite_tournaments} by introducing a
social-choice-style framework and several axioms for the problems of
\emph{truth discovery} and \emph{bipartite tournament ranking}. \todo{explain
what these are.}

As it turns out, many existing axioms from different problems in the literature
can be applied in our settings; if not directly, then the underlying idea can
be adapted. By looking at the consequences of such axioms we obtain insights
into the similarities and differences between our problems and other social
choice problems. \todo{explicit contributions?}

\section{Logic-based Perspectives}
\label{intro_sec_logic_based_perspectives}

\paragraph{Modal logic.}

{

\input{chapters/expertise/macros.tex}

In the logic-based part of the thesis, we start with a \emph{modal logic}
framework for reasoning about expertise. A modal language augments
propositional logic with one or more \emph{modalities}, which qualify
the truth value of a proposition~\cite{seplogicmodal}. A typical interpretation
is \emph{necessity}: $\modalnec\phi$ means the proposition $\phi$ is
\emph{necessarily true}, as opposed to merely being true. The dual notion of
\emph{possibility}, denoted $\modalposs\phi$, is defined in terms of necessity
by $\modalposs\phi \equiv \neg\modalnec\neg\phi$. That is, $\phi$ is
\emph{possibly true} if it is not necessarily false.

Various senses of ``necessity'' give rise to a rich landscape of logical
systems, useful for modelling various domains. For example, in temporal logics
$\modalnec\phi$ means $\phi$ will necessarily always hold in the future. In
deontic logics, $\modalnec\phi$ means $\phi$ is a moral necessity; it is
obligatory for $\phi$ to hold. In epistemic and doxastic logics,
$\modalnec\phi$ means $\phi$ is necessary from the point of view of an agent's
knowledge or beliefs about the world; one typically writes $\K\phi$ or
$\mathsf{B}\phi$ instead of $\modalnec\phi$ in these cases, to express that the
agent ``knows'' or ``believes'' $\phi$ \todo{cs}.

The most prominent semantic interpretation of modal formulas is based on
\emph{relational models} (also known as \emph{Kripke models}). The key
ingredients here are a set of \emph{states} and a binary relation called the
\emph{accessibility relation}. The modal formula $\modalnec\phi$ holds at a
state $x$ if $\phi$ holds at all states $y$ accessible from $x$.\footnotemark{}
In this way the accessibility relation directly reflects which states are
``possible'' from others.
%
Later, \emph{neighbourhood semantics} were developed by \textcite{Scott1970}
and \textcite{montague1970universal}, which generalise relational semantics.
The idea is to replace the accessibility relation with a so-called
\emph{neighbourhood function}, which assigns to each state a collection of sets
of states (called its \emph{neighbourhood}). This neighbourhood explicitly
lists the ``necessary'' propositions at each state: $\modalnec\phi$ holds at
$x$ if the set of states where $\phi$ holds is a member of the neighbourhood of
$x$~\cite{pacuit2017neighborhood}.
%
Further still, \emph{topological semantics} (also called the \emph{interior
semantics}) were first studied mathematically by \textcite{mckinsey41} and
\textcite{mckinseytarski44}, and later reinterpreted in epistemic terms (see
\textcite[Chapter 1]{ozgun_evidence} for a historical overview). Here one
equips the set states with a topology, and $\modalnec\phi$ holds at all points
in the \emph{interior} of the set where $\phi$ holds. That is, $\phi$ is
necessary at a state $x$ when there is an open neighbourhood of $x$ in which
$\phi$ holds at all points.

\footnotetext{
    A formal definition will be given in \cref{exp_sec_connection_with_ep_logic};
    for now we only wish to sketch the underlying ideas.
}

Relational, neighbourhood and topological semantics have each proven to be
useful for modelling notions related to our topic, such as information, trust,
belief, and evidence.

On trust, \textcite{Liau_2003} considered modalities $B_i\phi$ (agent $i$
believes $\phi$), $I_{ij}\phi$ ($i$ acquires information $\phi$ from $j$) and
$T_{ij}\phi$ ($i$ trusts $j$ on $\phi$), where trust has a neighbourhood
interpretation. \textcite{dastani2004inferring} extended this framework to
consider how trust may be inferred, using notions of topics and
\emph{questions}. \textcite{herzig2010logic} introduced notions of trust and
\emph{reputation}, in a framework where trust is not primitive but built from
beliefs, goals and actions. Further logical developments of trust were set out
by \textcite{rodenhauser2014matter} and \textcite{tagliaferri2019logical}; see
the references therein for a more thorough review of the literature on trust.

Interactions between knowledge, belief and \emph{evidence} have been studied in
epistemic logic. \textcite{moss1992topological} introduced the so-called subset
space semantics to model knowledge and \emph{epistemic effort}, which
represents a kind of evidence-gathering performed by an epistemic agent, and
has topological roots. \textcite{ozgun_evidence} further developed notions of
evidence in epistemic logic from a topological perspective. In a series of
papers, \textcite{van2011dynamic,van2012evidence,vanbenthem2014106} made
extensive use of neighbourhood structures to model evidence, and in particular
how inconsistent evidence can be combined to form beliefs.

The final strand of the modal literature we mention is \emph{dynamic epistemic
logic}~\cite{van_Ditmarsch_2008,sep_del}. Here modal operators describe
\emph{actions}: one has formulas of the form $[a]\phi$ to express that $\phi$
is true after the action $a$ is performed~\cite{sep_del}. Examples include
public announcements~\cite{plaza2007logics},
testimony~\cite{holliday2009dynamic} -- both particularly interesting in the
case of multiple agents -- belief revision~\cite{baltag2008qualitative} and
learning~\cite{gierasimczuk2009bridging,gierasimczuk2010knowing}.

Our work in \cref{chapter_expertise} combines elements from each of the
above-surveyed areas of the literature to model \emph{expertise} and its
relation to relation to \emph{truthfulness of information}. Specifically, we
introduce a logical system with two new modalities: $\E\phi$, meaning the
source in question is an expert on $\phi$, and $\S\phi$, meaning the
information $\phi$ is ``sound'' for the source to report. Informally, the
latter notion means $\phi$ is true \emph{up to lack of expertise}, i.e. if the
report becomes true when discarding parts of the statement on which the
reporting source lacks expertise.

For example, suppose an economist reports that energy policies proposed by the
government will stimulate economic growth and help tackle climate change. Since
this goes beyond the expertise of the economist (who we assume is not a climate
expert), we should only take their comments on the economy into account. Our
notion of soundness models this kind of filtering: whereas the statement in its
entirety may be false (e.g. if the proposed policies are not in fact
climate-friendly), the report is \emph{sound} whenever the economist is correct
on its economic content.

On the technical side, we use (a special case of) neighbourhood semantics for
expertise, and topological semantics for soundness. We show in detail how such
notions relate to knowledge in epistemic logic under relational semantics.
Dynamic operators are also considered; we define an analogue of public
announcements (called ``sound announcements'') and consider ``expertise
increase'', which models the effects of a source increasing their domain of
expertise by learning.

% different acccounts of trust.
% primitive:
%     liau
% non-primitive:
%     herzig (a logic of trust and reputation): built from beliefs, goals,
%       actions

}

\paragraph{Belief change.}

{
    \newcommand{\contract}{\dot{-}}
    \newcommand{\reviseby}{\ast}

Whereas \cref{chapter_expertise} proceeds in the tradition of modal epistemic
logic, \cref{chapter_belief_change} takes inspiration from the literature on
\emph{belief change}~\cite{booth_belief_2011,sep_belief_change,ferme_2018}.
This research area concerns how a rational agent should change its beliefs --
represented by a logically closed set of formulas $K$\footnotemark{} -- in
response to some operation. In belief \emph{contraction}, the agent must remove
some formula $\alpha$ from its belief set, obtaining new beliefs $K \contract
\alpha$. In belief \emph{revision}, the agent adjusts their beliefs to
incorporate $\alpha$, with the revised belief set denoted by $K \reviseby
\alpha$. Other operations include \emph{update}~\cite{katsuno_mendelzon_1992},
where the agent must change their beliefs to account for changes in the outside
world, and \emph{merging}, where inputs from several sources -- not necessarily
consistent with one another -- must be combined~\cite{konieczny2002merging}.

\footnotetext{
    But note that this is not the only way to represent beliefs. For iterated
    change, abstract ``epistemic states'' are used instead of belief
    sets~\cite{Darwiche_1997}. In belief \emph{base} change~\cite{hansson1999},
    ``belief bases'' are again sets of formulas but are not necessarily closed
    under logical consequence.
}

Note that unlike in epistemic and doxastic logics, where belief is represented
at the ``object-level'' via formulas $\mathsf{B}\phi$, belief change research
typically represents beliefs at the ``meta-level'' via sets of formulas
(typically propositional formulas).

A common principle guiding belief change methods is \emph{minimal change}: an
agent's belief after contraction/revision should remain as close to the initial
beliefs as possible. However, such minimal change may often be carried out in
several ways. For example, consider an agent who believes $p$ and $p \limplies
q$. Assuming beliefs are closed under logical consequence, the agent also
believes $q$. If the agent now learns $q$ is false, i.e. has to revise by
$\neg{q}$, there are two options: drop the belief in $p$ and retain $p
\limplies q$, or maintain $p$ but drop $p \limplies q$. From a purely logical
point of view, both are viable revision strategies. With this in mind, one
cannot single out a single contraction/revision operator. Instead, the dominant
approach is to set out \emph{rationality postulates} which constrain the
operation in question (much like the axioms of social choice theory, described
in \cref{intro_sec_social_choice_perspectives}). Additional structure (e.g.
preferences over formulas or propositional worlds) is then used to define
specific operators within the bounds of the postulates.

The seminal work of \textcite{alchourron1985logic} set out postulates for
contraction and revision. The influence of this work is such that belief
revision in this framework is now called \emph{AGM revision}, and the
postulates the \emph{AGM postulates}, named after its originators. We list the
revision postulates below.\footnotemark{}

\footnotetext{
    Note that some authors refer to the first six postulates as the ``basic
    postulates'' postulates, and the final two as the ``supplementary
    postulates''.  In this thesis, ``AGM postulates'' refers to all eight
    postulates.
}

\newcommand{\cn}[1]{\operatorname{Cn}\left({#1}\right)}

\begin{axiomlist}

    \begin{axiom}[\axiomref{Closure}]
        $K = \cn{K}$
    \end{axiom}

    \begin{axiom}[\axiomref{Success}]
        $\alpha \in K \reviseby \alpha$
    \end{axiom}

    \begin{axiom}[\axiomref{Inclusion}]
        $K \reviseby \alpha \subseteq \cn{K \cup \{\alpha\}}$
    \end{axiom}

    \begin{axiom}[\axiomref{Vacuity}]
        If $K \cup \{\alpha\}$ is consistent, then $K \reviseby \alpha
        = \cn{K \cup \{\alpha\}}$
    \end{axiom}

    \begin{axiom}[\axiomref{Consistency}]
        If $\alpha$ is consistent, then $K \reviseby \alpha$ is consistent
    \end{axiom}

    \begin{axiom}[\axiomref{Extensionality}]
        If $\alpha \equiv \beta$, then $K \reviseby \alpha = K \reviseby \beta$
    \end{axiom}

    \begin{axiom}[\axiomref{Subexpansion}]
        $K \reviseby (\alpha \land \beta) \subseteq \cn{(K \reviseby \alpha)
        \cup \{\beta\}}$
    \end{axiom}

    \begin{axiom}[\axiomref{Superexpansion}]
        If $(K \reviseby \alpha) \cup \{\beta\}$ is consistent, then $K
        \reviseby (\alpha \land \beta) \supseteq \cn{(K \reviseby \alpha) \cup
        \{\beta\}}$
    \end{axiom}

\end{axiomlist}

Note that \axiomref{Vacuity} goes some way to formalising the idea of minimal
change: if the $\alpha$ is already consistent with current beliefs, the agent
should just add $\alpha$ and close under logical consequence.

In our context, an important postulate which underlies the assumptions of the
AGM framework is \axiomref{Success}. As its name suggests, this says the
revision process was successful: $\alpha$ is indeed believed after revision by
$\alpha$. Consequently, the framework assumes the information by which to
revise is \emph{completely reliable}. While clearly useful in some contexts,
this severely limits the application scenarios of AGM revision.
\emph{Non-prioritised} revision subsequently arose to model situations where
the incoming information $\alpha$ is not prioritised over existing beliefs $K$.
Various approaches were surveyed by \textcite{hansson1999survey}. Typically
some extra structure accompanies a revision operator in order to decide to what
extent the new information is accepted. For instance, \emph{screened
revision}~\cite{makinson1997screened} maintains a subset $A \subseteq K$ of
``core'' beliefs which are untouchable; $\alpha$ is only accepted if it is
consistent with core beliefs. A similar construction for iterated revision was
proposed by \textcite{booth2005}. \emph{Selective
revision}~\cite{ferme1999selective} adds a pre-processing step to AGM revision:
a so-called ``selection function'' $f$ is used to weaken the incoming
information $\alpha$, and the revised belief set is $K \reviseby f(\alpha)$,
where $\reviseby$ is a (prioritised) AGM revision operator. In
\emph{credibility-limited revision}~\cite{hansson_2001} one considers a set
$\mathcal{C}$ of ``credible'' formulas; on receiving some $\alpha \in
\mathcal{C}$ one revises by $\alpha$ with an AGM operator as usual, but if
$\alpha \notin \mathcal{C}$ beliefs are unchanged.

Note that screened revision uses extra information about the \emph{agent},
whereas selective and credibility-limited revision use extra information about
the \emph{information source}. Indeed, the selection function $f$ can be used
to filter out parts of the input $\alpha$ on which the source is deemed to be
trustworthy. \textcite{booth_trust_2018} take this idea further, introducing a
specialisation of selective revision in which the selection function is derived
from an explicit representation of trust in the source. The idea is similar for
credibility-limited revision, where $\mathcal{C}$ represents the formulas on
which the source is trusted.

However, these approaches share a common deficiency: the non-prioritisation
mechanism remains fixed. That is, the trustworthiness of the reporting source
itself it not subject to revision. This is problematic in scenarios where
information sources are not well-known up front, and becomes especially
important when dealing with multiple sources. For example, consider conflicting
reports on a breaking news story posted on Twitter by unfamiliar users
$\mathsf{X}$ and $\mathsf{Y}$.  Initially we may be reluctant to commit to
believing either, not knowing much about their expertise. As time goes on,
however, more information becomes available. Perhaps a consensus emerges around
the report of $\mathsf{Y}$, or a report from a known, trusted source validates
$\mathsf{Y}$. In this case it may be natural to revise beliefs not only on the
news in question, but on the \emph{expertise of $\mathsf{X}$ and $\mathsf{Y}$
themselves}.

We take steps to resolve the situation by using the framework for expertise of
\cref{chapter_expertise} as the logical background for a belief change problem
with non-expert sources. Our operators take a sequence of reports from multiple
sources, and output a belief set in the extended language with expertise and
soundness modalities. In keeping with the AGM paradigm, beliefs and
trustworthiness are expressed on the meta-level. By using the extended language
of expertise we unify the trust and belief aspects in a common logical
language.\footnotemark{} Beliefs about the world are expressed as propositional
formulas as usual, and trust is expressed via \emph{belief in expertise}. That
is, a belief change agent trusts a source $\mathsf{X}$ on a formula $\phi$ if
$\mathsf{E}_{\mathsf{X}}\phi$ is included in its belief set.

\footnotetext{
    See the work of \textcite{yasser_21} -- which is discussed in detail in
    \cref{kr_sec_relatedwork} -- for an alternative approach in which trust and
    belief are treated separately.
}

Our approach is mainly postulational: we put forward a collection of postulates
for expetise-and-belief revision and offer a number of constructions satisfying
the postulates. Crucially we do \emph{not} require \axiomref{Success}, and our
operators cope with inconsistent reports. Formally, the framework is closer to
belief merging {\`a} la \textcite{konieczny2002merging} than AGM-style
revision, and a detailed comparison with merging is given in
\cref{kr_sec_relatedwork}.

\begin{notes}
    \begin{itemize}
        \item Other logical approaches to unreliable information: e.g.
              many-valued logics, possibilistic logic, rough sets (?).
    \end{itemize}
\end{notes}

\paragraph{Learning.}

While AGM revision tells us how to revise beliefs in a rational and minimal
way, it says nothing about whether the revised beliefs become \emph{closer to
the truth}. Indeed, belief revision theory does not include a model of the
``true'' state of the world, and thus the question of verisimilitude cannot be
addressed without extending the framework in some way.

On an intuitive level, the conservative principle of minimal change -- as
expressed by the AGM postulates -- may even conflict with pursuit of the truth:
sometimes radical changes are required. For example, consider a conservative
agent who strongly believes a biased coin is in fact fair. Then a sequence of
1000 consecutive heads, while vanishingly unlikely, is nevertheless
\emph{consistent} with a fair coin, and our agent will see no reason to change
beliefs.

Fortunately, not all AGM operators embody conservatism to this extreme extent.
\textcite{kelly1997reliable} analysed AGM revision from the point of view of
formal learning
theory~\cite{jain1999systems},\cite[\sectionsymbol{2.1}]{gierasimczuk2010knowing},
and showed that AGM operators are \emph{universal}, in the sense that if the
truth can be found by any learning method at all, it can be found by an AGM
operator. However, the initial beliefs need to be chosen carefully to avoid
situations like the one described above. Specific AGM operators from the
literature were also studied for their learning-theoretic properties in this
framework~\cite{kelly1998learning,Kelly_1999}. In a similar framework,
\textcite{gierasimczuk2010knowing,baltag_tt_2019,Baltag_2016} also studied
various belief revision methods in relation to learning, and proved
universality of AGM learning.

While our belief change operators in \cref{chapter_belief_change} are inspired
by the AGM paradigm, there is a crucial difference: we expressly aim to handle
non-expert, and therefore \emph{false}, information. \todo{finish}

}

\begin{notes} \begin{itemize}
        \item Inductive reasoning and learning
        \item e.g. Kevin Kelly's work, Nina \end{itemize} \end{notes}

\section{Contributions}

\todo{Consolidate contribution paragraphs here, and present in bullet form.}

\section{Overview}

\begin{notes}
    Chapter-by-chapter breakdown of the thesis.
\end{notes}
