\chapter{Introduction}

The overall theme of this thesis is \emph{unreliable information}. How should
unreliable information be aggregated? Who should be trusted when conflicts
arise between unreliable sources? And what, if anything, can be \emph{learned}
from non-expert information? These are the central issues the thesis aims to
address.

Indeed, methods for understanding and reasoning with unreliable information are
becoming ever more relevant in today's world, as the volume of data produced
and consumed grows year-on-year. With the growth of user-generated
content on the internet, most prominently on social media platforms, false
information can spread rapidly -- sometimes with dramatic
consequences As such, much research effort has gone into identifying
false information, estimating source reliability, and understanding the nature
of how people may come to believe and share false information.

The gap this thesis aims to fill concerns \emph{formal models} of problems
surrounding unreliable information. We take a mathematical approach, putting
forward formal frameworks in which the relevant problems can be formulated
precisely. In doing so we obtain conceptual results which aim to shine light on
the core, abstract features of such problems. It is hoped that the thesis
complements the various empirical, practical and philosophical approaches
to our topic, and contributes to the broader understanding of
trustworthiness, expertise and unreliable information from a mathematical point
of view.

The thesis is split into two parts along methodological lines. In
\cref{chapter_td,chapter_bipartite_tournaments} we use the tools and ideas of
\emph{computational social choice theory}~\cite{brandt2016introduction} to
explore the problems of aggregating unreliable information and ranking sources
by trustworthiness.
%
\cref{chapter_expertise,chapter_belief_change,chapter_truthtracking} take a
\emph{logic-based} approach. We develop a modal logic framework to give precise
semantics for expertise, and explore the connection between expertise,
knowledge, and the truthfulness of information. This framework is used as the
foundation for a belief change problem in the tradition of knowledge
representation and rational belief
change~\cite{booth_belief_2011,sep_belief_change,ferme_2018}. Finally, we
combine ideas from \emph{formal learning
theory}~\cites{jain1999systems}[\sectionsymbol{2.1}]{gierasimczuk2010knowing}
(and in particular, the intersection of formal learning theory and belief
revision~\cite{baltag_tt_2019}) to investigate the extent to which one can
\emph{learn} from unreliable information.
%
In the remainder of this chapter we briefly survey the literature for both
halves.

\section{Social Choice Perspectives}
\label{intro_sec_social_choice_perspectives}

Broadly speaking, social choice theory is the study of \emph{aggregating
preferences}. The prototypical example is \emph{voting}. In an election, each
member of the electorate submits a vote in the form of their preferences over
the candidates. A voting rule then aggregates these preferences into a
\emph{collective decision} by declaring the winning candidate, the runner-up,
and so on. There are, of course, many different voting methods which can be
used to aggregate votes, and several distinct methods are in use in different
contexts across the world.

In analysing and comparing such methods, the \emph{axiomatic approach} has been
a crucial methodological tool since the seminal work of \textcite{arrow1952},
who initiated the age of so-called \emph{classical social choice theory}. In
taking this approach, one formalises intuitively desirable properties called
\emph{axioms}, which are expected to hold for ``reasonable'' aggregation
methods. This provides a \emph{normative} basis on which to construct and judge
voting rules. The benefits of this approach were already shown by Arrow, who
proved, surprisingly, that it is mathematically impossible for any voting
method to simultaneously satisfy a short list of seemingly desirable axioms.
This type of result -- known as an \emph{impossibility theorem} -- highlights a
fundamental and inescapable property of voting.\footnotemark{} This has
practical consequences: if one needs to actually choose a rule for use in a
vote, which axiom will be sacrificed?

\footnotetext{
    At least, a property of voting in the sense of Arrow's formal framework.
}

Axiomatic analysis can also be applied in a \emph{descriptive} context, where
one starts with a known voting rule and aims to find axioms satisfied by it. In
many cases a set of axioms can be found to characterise a particular rule
completely, in that it is the \emph{unique} rule satisfying them. This gives
additional insight into the nature of the rule and in how different axioms
interact.

The axiomatic approach has since been adapted to various domains besides
voting, including tournaments~\cite{brandt2016a}, judgement
aggregation~\cite{endriss2016ja}, the ranking of web
pages~\cite{altman2005ranking}, reputation systems~\cite{tennenholtz2004} and
collective annotation~\cite{kruger2014}. While the aggregation problems of each
domain have their own unique characteristics, some ``standard'' axioms appear
across the board. While the precise mathematical formulation of such axioms
varies across problems, the general intent is the same. We use the example of
voting to illustrate a few. In this setting each voter submits a ranking over
the set of candidates (the voter's \emph{ballot}), which a voting rule
aggregates to form a collective ranking.

\begin{axiomlist}

    \begin{axiom}[\axiomref{Anonymity}]
        All voters are treated equally: if voters $i$ and $j$ swap their
        ballots, the collective ranking remains the same.
    \end{axiom}

    \begin{axiom}[\axiomref{Neutrality}]
        All candidates are treated equally: if all voters swap the positions of
        candidates $c$ and $d$ in their ballots, the positions of $c$ and $d$
        are also swapped in the collective ranking.
    \end{axiom}

    \begin{axiom}[\axiomref{Pareto Optimality}]
        If all voters rank candidate $c$ strictly above $d$, then so too does
        the collective ranking.
    \end{axiom}

    \begin{axiom}[\axiomref{Independence of Irrelevant Alternatives (IIA)}]
        The relative ranking of any two candidates $c$ and $d$ in the
        collective ranking depends only on the rankings of $c$ and $d$ in each
        voter's ballot.
    \end{axiom}

    \begin{axiom}[\axiomref{Positive Responsiveness}]
        If $c$ ranks above $d$ in the collective ranking and some voter changes
        their ballot to rank $c$ above $d$, then this remains so in the
        collective ranking.
    \end{axiom}

\end{axiomlist}

Some of these axioms are more clearly desirable than others.
\axiomref{Anonymity} and \axiomref{Neutrality} are fairly straightforward
fairness requirements\footnotemark{}, and \axiomref{Pareto Optimality} is
generally seen as uncontroversial. \axiomref{IIA}, however, has come to be seen
as a deceptively strong requirement, and plays a role in Arrow's impossibility
theorem.

\footnotetext{
    Although there are situations where even these do not hold; see
    \textcite[p. 32]{zwicker2016voting} for examples.
}

In the last 20 years, \emph{computational social choice}~\cite{moulin_2016} has
combined social choice theory with ideas from theoretical computer science. For
example, complexity theory has been used to show certain voting rules are
resistant to strategic manipulation~\cite{conitzer_manipulation_2016},
approximation algorithms have been developed for computationally difficult
rules~\cite{brandt2016a}, and SAT solvers have been used to automatically
discover new impossibility theorems~\cite{endriss2020analysis}.

In the spirit of combining the axiomatic approach with computer science, we
proceed in \cref{chapter_td,chapter_bipartite_tournaments} by introducing a
social-choice-style framework and several axioms for the problems of
\emph{truth discovery} and \emph{bipartite tournament ranking}.

\paragraph{Truth discovery.}

Truth discovery has arisen recently as a branch of the literature on
crowdsourcing~\cite{li_survey_2016}. When dealing with crowdsourced data one
has no guarantees on its veracity: crowdsourcing workers are not generally
experts, may exhibit biases, and may even maliciously provide false
information. False information provided in this way leads to conflicts among
workers, and the dual goals of truth discovery: to find the \emph{true
information} in light of conflicts, and to identify the \emph{trustworthy
information sources}. The key principle underlying truth discovery methods is
the \emph{mutual dependence} between these two goals. According to this
principle, sources that provide true information are likely to be trustworthy,
and information from trustworthy sources is likely to be true. When sources
provide information about multiple objects of interest (e.g. images to be
classified), the patterns of agreement and disagreement can be used in
conjunction with this principle to estimate both truth and trustworthiness.

Most truth discovery work in the literature has focussed on introducing new
methods and evaluating them empirically.\footnotemark{} Our aim in
\cref{chapter_td} is to instead study properties of truth discovery methods
more generally, by emphasising the similarities between truth discovery and
social choice aggregation problems. To that end, we introduce a mathematical
framework in which social-choice-style axioms can be expressed. We then explore
the interplay between the axioms, and analyse a particular well-known method
from the literature.

\footnotetext{
    A more detailed overview of the literature will be given in
    \cref{chapter_td}.
}

\paragraph{Bipartite tournaments.}

While many truth discovery methods in the literature focus on the unsupervised
case, so-called \emph{semi-supervised truth discovery} -- in which one has
access to a subset of ground truth data -- has also been
studied~\cite{yin_supervised_2011,rekatsinas2017slimfast}. In this situation it
is known when sources were correct on the ground truth objects, and this
information feeds into the truth discovery process. Using this ground truth may
not be straightforward, however, if objects vary in their \emph{difficulty}.
For example, it may be preferable to reward sources for correct answers on
difficult objects, or penalise them for failures on easy objects. Moreover,
difficulty may not be known \emph{a priori}, and is itself subject to
estimation.

In \cref{chapter_bipartite_tournaments} we observe and generalise the key
features of this problem: we have entities of two different kinds (information
sources and objects of interest), comparisons between them (a source is either
correct or incorrect on the object) and wish to determine a ranking among each
kind (sources by trustworthiness, and objects by their difficulty). These two
rankings should express a kind of mutual dependence -- reminiscent of the
principle of truth discovery described above -- wherein, for example, sources
correct on difficult objects should rank highly. A novel aspect of this
particular problem is that there are no \emph{direct} comparisons between
entities of the same kind: sources to not go head-to-head, but must be ranked
based on \emph{indirect} patterns of correctness across objects.

This problem is an instance of a \emph{tournament}. Tournaments consists of a
set of \emph{players} together with a \emph{beating relation} between them, and
have been widely studied in social choice theory. Obvious application domains
include sports, but tournaments can be applied more widely (e.g. in voting,
where candidate $c$ ``beats'' $d$ if a majority of voters prefer $c$ to $d$).
Many tournament solutions -- producing either a set of winners or a full
ranking of the players -- have been proposed in the literature and studied
axiomatically~\cite{gonzalez2014paired,brandt2016a}.

While our problem can be seen as a tournament, the bipartite nature and
indirectness of comparisons between players to be ranked gives it a unique
character worthy of dedicated study. We focus on a particular class of
intuitive bipartite tournament ranking methods based on \emph{chain editing}; a
graph modification problem mainly studied for its computational complexity
properties~\cite{yannakakis1981computing} and recently suggested in the context
of ranking by \textcite{jiao2017algorithms}. While their work focussed on
algorithms and the complexity of variants of chain editing, we take an
axiomatic approach in the social choice tradition in order to understand its
properties as a ranking mechanism.

\section{Logic-based Perspectives}
\label{intro_sec_logic_based_perspectives}

\paragraph{Modal logic.}

{

\input{chapters/expertise/macros.tex}

In the logic-based part of the thesis, we start with a \emph{modal logic}
framework for reasoning about expertise. A modal language augments
propositional logic with one or more \emph{modalities}, which qualify
the truth value of a proposition~\cite{seplogicmodal}. A typical interpretation
is \emph{necessity}: $\modalnec\phi$ means the proposition $\phi$ is
\emph{necessarily true}, as opposed to merely being true. The dual notion of
\emph{possibility}, denoted $\modalposs\phi$, is defined in terms of necessity
by $\modalposs\phi \equiv \neg\modalnec\neg\phi$. That is, $\phi$ is
\emph{possibly true} if it is not necessarily false.

Various senses of ``necessity'' give rise to a rich landscape of logical
systems, useful for modelling various domains. For example, in temporal logics
$\modalnec\phi$ means $\phi$ will necessarily always hold in the
future~\cite{goranko_temporal}. In deontic logics, $\modalnec\phi$ means $\phi$
is a moral necessity; it is obligatory for $\phi$ to
hold~\cite{mcnama_deontic}. In epistemic and doxastic logics, $\modalnec\phi$
means $\phi$ is necessary from the point of view of an agent's knowledge or
beliefs about the world; one typically writes $\K\phi$ or $\mathsf{B}\phi$
instead of $\modalnec\phi$ in these cases, to express that the agent ``knows''
or ``believes'' $\phi$~\cite{rendsvig_epistemic}.

The most prominent semantic interpretation of modal formulas is based on
\emph{relational models} (also known as \emph{Kripke models}). The key
ingredients here are a set of \emph{states} and a binary relation called the
\emph{accessibility relation}. The modal formula $\modalnec\phi$ holds at a
state $x$ if $\phi$ holds at all states $y$ accessible from $x$.\footnotemark{}
In this way the accessibility relation directly reflects which states are
``possible'' from others.
%
Later, \emph{neighbourhood semantics} were developed by \textcite{Scott1970}
and \textcite{montague1970universal}, which generalise relational semantics.
The idea is to replace the accessibility relation with a so-called
\emph{neighbourhood function}, which assigns to each state a collection of sets
of states (called its \emph{neighbourhood}). This neighbourhood explicitly
lists the ``necessary'' propositions at each state: $\modalnec\phi$ holds at
$x$ if the set of states where $\phi$ holds is a member of the neighbourhood of
$x$~\cite{pacuit2017neighborhood}.
%
Further still, \emph{topological semantics} (also called the \emph{interior
semantics}) were first studied mathematically by \textcite{mckinsey41} and
\textcite{mckinseytarski44}, and later reinterpreted in epistemic terms (see
\textcite[Chapter 1]{ozgun_evidence} for a historical overview). Here one
equips the set states with a topology, and $\modalnec\phi$ holds at all points
in the \emph{interior} of the set where $\phi$ holds. That is, $\phi$ is
necessary at a state $x$ when there is an open neighbourhood of $x$ in which
$\phi$ holds at all points.

\footnotetext{
    A formal definition will be given in \cref{exp_sec_connection_with_ep_logic};
    for now we only wish to sketch the underlying ideas.
}

Relational, neighbourhood and topological semantics have each proven to be
useful for modelling notions related to our topic, such as information, trust,
belief, and evidence.

On trust, \textcite{Liau_2003} considered modalities $B_i\phi$ (agent $i$
believes $\phi$), $I_{ij}\phi$ ($i$ acquires information $\phi$ from $j$) and
$T_{ij}\phi$ ($i$ trusts $j$ on $\phi$), where trust has a neighbourhood
interpretation. \textcite{dastani2004inferring} extended this framework to
consider how trust may be inferred, using notions of topics and
\emph{questions}. \textcite{herzig2010logic} introduced notions of trust and
\emph{reputation}, in a framework where trust is not primitive but built from
beliefs, goals and actions. Further logical developments of trust were set out
by \textcite{rodenhauser2014matter} and \textcite{tagliaferri2019logical}; see
the references therein for a more thorough review of the literature on trust.

Interactions between knowledge, belief and \emph{evidence} have been studied in
epistemic logic. \textcite{moss1992topological} introduced the so-called subset
space semantics to model knowledge and \emph{epistemic effort}, which
represents a kind of evidence-gathering performed by an epistemic agent, and
has topological roots. \textcite{ozgun_evidence} further developed notions of
evidence in epistemic logic from a topological perspective. In a series of
papers, \textcite{van2011dynamic,van2012evidence,vanbenthem2014106} made
extensive use of neighbourhood structures to model evidence, and in particular
how inconsistent evidence can be combined to form beliefs.

The final strand of the modal literature we mention is \emph{dynamic epistemic
logic}~\cite{van_Ditmarsch_2008,sep_del}. Here modal operators describe
\emph{actions}: one has formulas of the form $[a]\phi$ to express that $\phi$
is true after the action $a$ is performed~\cite{sep_del}. Examples include
public announcements~\cite{plaza2007logics},
testimony~\cite{holliday2009dynamic} -- both particularly interesting in the
case of multiple agents -- belief revision~\cite{baltag2008qualitative} and
learning~\cite{gierasimczuk2009bridging,gierasimczuk2010knowing}.

Our work in \cref{chapter_expertise} combines elements from each of the
above-surveyed areas of the literature to model \emph{expertise} and its
relation to relation to \emph{truthfulness of information}. Specifically, we
introduce a logical system with two new modalities: $\E\phi$, meaning the
source in question is an expert on $\phi$, and $\S\phi$, meaning the
information $\phi$ is ``sound'' for the source to report. Informally, the
latter notion means $\phi$ is true \emph{up to lack of expertise}, i.e. if the
report becomes true when discarding parts of the statement on which the
reporting source lacks expertise.

For example, suppose an economist reports that energy policies proposed by the
government will stimulate economic growth and help tackle climate change. Since
this goes beyond the expertise of the economist (who we assume is not a climate
expert), we should only take their comments on the economy into account. Our
notion of soundness models this kind of filtering: whereas the statement in its
entirety may be false (e.g. if the proposed policies are not in fact
climate-friendly), the report is \emph{sound} whenever the economist is correct
on its economic content.

On the technical side, we use (a special case of) neighbourhood semantics for
expertise, and topological semantics for soundness. We show in detail how such
notions relate to knowledge in epistemic logic under relational semantics.
Dynamic operators are also considered; we define an analogue of public
announcements (called ``sound announcements'') and consider ``expertise
increase'', which models the effects of a source increasing their domain of
expertise by learning.

We also obtain axiomatisation results for various classes of expertise models.
Note that ``axiom'' here has a different meaning to the axioms of social choice
theory. In a logical system, axioms are formulas which -- together with rules
of inferrence -- give rise to a notion of \emph{syntactic entailment} or
\emph{proof}: one writes $\Gamma \entails \phi$ if $\phi$ can be derived using
the axioms and inferrence rules from the assumptions in $\Gamma$. Axioms
therefore form the building blocks of (synactic) reasoning in a logic. There is
also the dual notion of \emph{semantic entailment}: $\Gamma \models \phi$ if
for every model and every state $x$, if all formulas in $\Gamma$ hold at $x$
then $\phi$ also holds at $x$.

The task of choosing axioms such that these two notions of entailment coincide
is precisely what it means to find an axiomatisation. The implication $\Gamma
\entails \phi \implies \Gamma \models \phi$ is called \emph{soundness}, and
says that anything one can prove syntactically is actually true according to
the semantics. The converse implication $\Gamma \models \phi \implies \Gamma
\entails \phi$ is called (strong) \emph{completeness};\footnotemark{} this
direction is typically more difficult to prove, and says that anything which is
true semantically can indeed be proved.

\footnotetext{
    \emph{Weak} completeness is the special case of strong completeness in
    which $\Gamma = \emptyset$.
}

By restricting to a sub-class of models, one obtains a different (and stronger)
notion of semantic entailment. For example, one has $\modalnec\phi \models
\modalnec\modalnec\phi$ when restricting to the class of \emph{transitive}
relational models, but not in general. Generally speaking, restricting the
class of models under consideration requires more axioms to ensure
completeness. In our case, we will consider several classes of expertise models
in which various assumptions are placed on the nature of the source's expertise
(for instance, the ways in which they may combine their expertise on separate
pieces of information), and introduce additional axioms in each case.

}

\paragraph{Belief change.}

{
    \newcommand{\contract}{\dot{-}}
    \newcommand{\reviseby}{\ast}

Whereas \cref{chapter_expertise} proceeds in the tradition of modal epistemic
logic, \cref{chapter_belief_change} takes inspiration from the literature on
\emph{belief change}~\cite{booth_belief_2011,sep_belief_change,ferme_2018}.
This research area concerns how a rational agent should change its beliefs --
represented by a logically closed set of formulas $K$\footnotemark{} -- in
response to some operation. In belief \emph{contraction}, the agent must remove
some formula $\alpha$ from its belief set, obtaining new beliefs $K \contract
\alpha$. In belief \emph{revision}, the agent adjusts their beliefs to
incorporate $\alpha$, with the revised belief set denoted by $K \reviseby
\alpha$. Other operations include \emph{update}~\cite{katsuno_mendelzon_1992},
where the agent must change their beliefs to account for changes in the outside
world, and \emph{merging}, where inputs from several sources -- not necessarily
consistent with one another -- must be combined~\cite{konieczny2002merging}.

\footnotetext{
    But note that this is not the only way to represent beliefs. For iterated
    change, abstract ``epistemic states'' are used instead of belief
    sets~\cite{Darwiche_1997}. In belief \emph{base} change~\cite{hansson1999},
    ``belief bases'' are again sets of formulas but are not necessarily closed
    under logical consequence.
}

Note that unlike in epistemic and doxastic logics, where belief is represented
at the ``object-level'' via formulas $\mathsf{B}\phi$, belief change research
typically represents beliefs at the ``meta-level'' via sets of formulas
(typically propositional formulas).

A common principle guiding belief change methods is \emph{minimal change}: an
agent's belief after contraction/revision should remain as close to the initial
beliefs as possible. However, such minimal change may often be carried out in
several ways. For example, consider an agent who believes $p$ and $p \limplies
q$. Assuming beliefs are closed under logical consequence, the agent also
believes $q$. If the agent now learns $q$ is false, i.e. has to revise by
$\neg{q}$, there are two options: drop the belief in $p$ and retain $p
\limplies q$, or maintain $p$ but drop $p \limplies q$. From a purely logical
point of view, both are viable revision strategies. With this in mind, one
cannot single out a single contraction/revision operator. Instead, the dominant
approach is to set out \emph{rationality postulates} which constrain the
operation in question (much like the axioms of social choice theory, described
in \cref{intro_sec_social_choice_perspectives}). Additional structure (e.g.
preferences over formulas or propositional worlds) is then used to define
specific operators within the bounds of the postulates.

The seminal work of \textcite{alchourron1985logic} set out postulates for
contraction and revision. The influence of this work is such that belief
revision in this framework is now called \emph{AGM revision}, and the
postulates the \emph{AGM postulates}, named after its originators. For a fixed
belief set $K$, the postulates are as follows.\footnotemark{}

\footnotetext{
    Note that some authors refer to the first six postulates as the ``basic
    postulates'' postulates, and the final two as the ``supplementary
    postulates''.  In this thesis, ``AGM postulates'' refers to all eight
    postulates.
}

\newcommand{\cn}[1]{\operatorname{Cn}\left({#1}\right)}

\begin{axiomlist}

    \begin{axiom}[\axiomref{Closure}]
        $K = \cn{K}$
    \end{axiom}

    \begin{axiom}[\axiomref{Success}]
        $\alpha \in K \reviseby \alpha$
    \end{axiom}

    \begin{axiom}[\axiomref{Inclusion}]
        $K \reviseby \alpha \subseteq \cn{K \cup \{\alpha\}}$
    \end{axiom}

    \begin{axiom}[\axiomref{Vacuity}]
        If $K \cup \{\alpha\}$ is consistent, then $K \reviseby \alpha
        = \cn{K \cup \{\alpha\}}$
    \end{axiom}

    \begin{axiom}[\axiomref{Consistency}]
        If $\alpha$ is consistent, then $K \reviseby \alpha$ is consistent
    \end{axiom}

    \begin{axiom}[\axiomref{Extensionality}]
        If $\alpha \equiv \beta$, then $K \reviseby \alpha = K \reviseby \beta$
    \end{axiom}

    \begin{axiom}[\axiomref{Subexpansion}]
        $K \reviseby (\alpha \land \beta) \subseteq \cn{(K \reviseby \alpha)
        \cup \{\beta\}}$
    \end{axiom}

    \begin{axiom}[\axiomref{Superexpansion}]
        If $(K \reviseby \alpha) \cup \{\beta\}$ is consistent, then $K
        \reviseby (\alpha \land \beta) \supseteq \cn{(K \reviseby \alpha) \cup
        \{\beta\}}$
    \end{axiom}

\end{axiomlist}

An operator $\reviseby$ satisfying the postulates is called an \emph{AGM
operator} for $K$. Here $\cn{\cdot}$ denotes logical consequence.
%
Note that \axiomref{Vacuity} goes some way to formalising the idea of minimal
change: if the $\alpha$ is already consistent with current beliefs, the agent
should just add $\alpha$ and close under logical consequence.

In our context, an important postulate which underlies the assumptions of the
AGM framework is \axiomref{Success}. As its name suggests, this says the
revision process was successful: $\alpha$ is indeed believed after revision by
$\alpha$. Consequently, the framework assumes the information by which to
revise is \emph{completely reliable}. While clearly useful in some contexts,
this severely limits the application scenarios of AGM revision.
\emph{Non-prioritised} revision subsequently arose to model situations where
the incoming information $\alpha$ is not prioritised over existing beliefs $K$.
Various approaches were surveyed by \textcite{hansson1999survey}. Typically
some extra structure accompanies a revision operator in order to decide to what
extent the new information is accepted. For instance, \emph{screened
revision}~\cite{makinson1997screened} maintains a subset $A \subseteq K$ of
``core'' beliefs which are untouchable; $\alpha$ is only accepted if it is
consistent with core beliefs. A similar construction for iterated revision was
proposed by \textcite{booth2005}. \emph{Selective
revision}~\cite{ferme1999selective} adds a pre-processing step to AGM revision:
a so-called ``selection function'' $f$ is used to weaken the incoming
information $\alpha$, and the revised belief set is $K \reviseby f(\alpha)$,
where $\reviseby$ is a (prioritised) AGM revision operator. In
\emph{credibility-limited revision}~\cite{hansson_2001} one considers a set
$\mathcal{C}$ of ``credible'' formulas; on receiving some $\alpha \in
\mathcal{C}$ one revises by $\alpha$ with an AGM operator as usual, but if
$\alpha \notin \mathcal{C}$ beliefs are unchanged.

Note that screened revision uses extra information about the \emph{agent},
whereas selective and credibility-limited revision use extra information about
the \emph{information source}. Indeed, the selection function $f$ can be used
to filter out parts of the input $\alpha$ on which the source is deemed to be
trustworthy. \textcite{booth_trust_2018} take this idea further, introducing a
specialisation of selective revision in which the selection function is derived
from an explicit representation of trust in the source. The idea is similar for
credibility-limited revision, where $\mathcal{C}$ represents the formulas on
which the source is trusted.

However, these approaches share a common deficiency: the non-prioritisation
mechanism remains fixed. That is, the trustworthiness of the reporting source
itself it not subject to revision. This is problematic in scenarios where
information sources are not well-known up front, and becomes especially
important when dealing with multiple sources. For example, consider conflicting
reports on a breaking news story posted on Twitter by unfamiliar users
$\mathsf{X}$ and $\mathsf{Y}$.  Initially we may be reluctant to commit to
believing either, not knowing much about their expertise. As time goes on,
however, more information becomes available. Perhaps a consensus emerges around
the report of $\mathsf{Y}$, or a report from a known, trusted source validates
$\mathsf{Y}$. In this case it may be natural to revise beliefs not only on the
news in question, but on the \emph{expertise of $\mathsf{X}$ and $\mathsf{Y}$
themselves}.

We take steps to resolve the situation by using the framework for expertise of
\cref{chapter_expertise} as the logical background for a belief change problem
with non-expert sources. Our operators take a sequence of reports from multiple
sources, and output a belief set in the extended language with expertise and
soundness modalities. In keeping with the AGM paradigm, beliefs and
trustworthiness are expressed on the meta-level. By using the extended language
of expertise we unify the trust and belief aspects in a common logical
language.\footnotemark{} Beliefs about the world are expressed as propositional
formulas as usual, and trust is expressed via \emph{belief in expertise}. That
is, a belief change agent trusts a source $\mathsf{X}$ on a formula $\phi$ if
$\mathsf{E}_{\mathsf{X}}\phi$ is included in its belief set.

\footnotetext{
    See the work of \textcite{yasser_21} -- which is discussed in detail in
    \cref{kr_sec_relatedwork} -- for an alternative approach in which trust and
    belief are treated separately.
}

Our approach is mainly postulational: we put forward a collection of postulates
for expetise-and-belief revision and offer a number of constructions satisfying
the postulates. Crucially we do \emph{not} require \axiomref{Success}, and our
operators cope with inconsistent reports. Formally, the framework is closer to
belief merging {\`a} la \textcite{konieczny2002merging} than AGM-style
revision, and a detailed comparison with merging is given in
\cref{kr_sec_relatedwork}.

\paragraph{Learning.}

While AGM revision tells us how to revise beliefs in a rational and minimal
way, it says nothing about whether the revised beliefs become \emph{closer to
the truth}. Indeed, belief revision theory does not include a model of the
``true'' state of the world, and thus the question of verisimilitude cannot be
addressed without extending the framework in some way.

On an intuitive level, the conservative principle of minimal change -- as
expressed by the AGM postulates -- may even conflict with pursuit of the truth:
sometimes radical changes are required~\cite{kelly1997reliable}. For example,
consider a conservative agent who strongly believes a biased coin is in fact
fair. Then a sequence of 1000 consecutive heads, while vanishingly unlikely, is
nevertheless \emph{consistent} with a fair coin, and our agent will see no
reason to change beliefs.

Fortunately, not \emph{all} AGM operators embody conservatism to this extreme
extent. \textcite{kelly1997reliable} analysed AGM revision from the point of
view of formal learning
theory~\cites{jain1999systems}[\sectionsymbol{2.1}]{gierasimczuk2010knowing},
and showed that AGM operators are \emph{universal}, in the sense that if the
truth can be found by any learning method at all, it can be found by an AGM
operator. However, the initial beliefs need to be chosen carefully to avoid
situations like the one described above. Specific AGM operators from the
literature were also studied for their learning-theoretic
properties~\cite{kelly1998learning,Kelly_1999}. In a similar framework,
\textcite{gierasimczuk2010knowing,baltag_tt_2019,Baltag_2016} studied various
belief revision methods in relation to learning, and again proved universality
of AGM learning.

While our belief change operators in \cref{chapter_belief_change} are inspired
by the AGM paradigm, there is a crucial difference: we expressly aim to handle
non-expert -- and therefore \emph{false} -- information. In
\cref{chapter_truthtracking} we adapt the truth-tracking idea of
\textcite{baltag_tt_2019} to this new setting, and formalise the notion of
\emph{learning} from non-expert sources. We will investigate the extent to
which learning is actually possible, both in terms of learning the true facts
of the world and learning the true extent of the expertise of the sources
themselves. In this respect the learning problem is similar to truth discovery,
as studied in \cref{chapter_td}. A universality result of sorts is also
available in our setting, and we offer a precise postulational characterisation
of such truth-tracking methods. It turns out the some (but not all) of our
example operators from the belief change problem in
\cref{chapter_belief_change} are truth-tracking; this shows that the
rationality postulates \emph{are} compatible with learning from non-experts.

}

\todo{
    Consider mentioning other logical approaches to unreliable information:
    e.g.  many-valued logics, possibilistic logic, rough sets (?).
}

\section{Contributions}

The novel contributions of this thesis may be broken down per-chapter as
follows.

\paragraph{\cref{chapter_td}.}
\begin{itemize}
    \item A formal framework for truth discovery suitable for axiomatic
          analysis in the style of social choice theory.
    \item The introduction of several axioms in this framework, the first
          impossibility theorems for truth discovery, and axiomatic analysis of
          a well-known truth discovery method from the literature.
\end{itemize}

\paragraph{\cref{chapter_bipartite_tournaments}.}
\begin{itemize}
    \item The definition of a new class of \emph{bipartite tournaments} and the
          associated ranking problem.
    \item An in-depth study of \emph{chain editing} as a ranking method for
          bipartite tournaments, including axiomatic analysis in the style of
          social choice theory.
\end{itemize}

\paragraph{\cref{chapter_expertise}.}
\begin{itemize}
    \item A modal logic framework to reason about the expertise of an
          information source, and ``soundness'' of information.
    \item Results establishing the connection between expertise and
          epistemic logic, so that expertise can be interpreted in terms of
          \emph{knowledge}.
    \item Technical results on the mathematical properties of the logic of
          expertise, including axiomatisation results for several classes of
          expertise models.
\end{itemize}

\paragraph{\cref{chapter_belief_change}.}
\begin{itemize}
    \item The statement of a new logic-based belief change problem for handling
          reports from multiple non-expert sources, in which we aim to
          determine what to believe both about the world and about the
          expertise of the sources themselves.
    \item New postulates and operators for this problem.
\end{itemize}

\paragraph{\cref{chapter_truthtracking}.}
\begin{itemize}
    \item A framework for truth-tracking with multiple non-expert sources,
          adapted from previous work from formal learning theory and belief
          revision.
    \item Mathematical analysis of when truth-tracking is possible with
          non-experts, and the extent to which one can learn the truth.
\end{itemize}
